# ğŸ§  Workana Data Engineer - Technical Test

Este repositorio contiene la estructura base para resolver la prueba tÃ©cnica de Data Engineer para Workana, utilizando PostgreSQL como base de datos.

## ğŸ“ Estructura del Proyecto

- `airflow_dags/`: DAGs de Airflow para orquestaciÃ³n local
- `etl/`: Scripts ETL en Python
- `sql/`: Consultas SQL y scripts de creaciÃ³n
- `modeling/`: Esquema dimensional y scripts relacionados
- `data/`: Datos de entrada y salida
- `ci/`: Archivos de configuraciÃ³n para CI/CD
- `tests/`: Pruebas unitarias e integraciÃ³n
- `logs/` y `output/`: Salidas del sistema y resultados

## âš™ï¸ Requisitos

- Python 3.12
- SQLite
- Airflow (opcionalmente en Docker)
- LibrerÃ­as: ver `requirements.txt`

## â–¶ï¸ Uso rÃ¡pido

```bash
python3 -m venv .venv
source venv/bin/activate
pip install -r requirements.txt
```

## ğŸ§ª Pruebas

```bash
pytest tests/
```

## ğŸš€ OrquestaciÃ³n

Airflow o alternativa compatible. Inicia los DAGs desde `airflow_dags/`.

## ğŸ“Œ Notas

Este proyecto usa SQLite como base de datos por defecto.

# âœ… Plan de AcciÃ³n: Ejercicio 1 - OrquestaciÃ³n local

## ğŸ¯ Objetivo

Crear un workflow end-to-end local que procese `sample_transactions.csv`, lo transforme con Python y lo cargue a una base de datos local (SQLite o PostgreSQL), usando una herramienta de orquestaciÃ³n como Airflow.

---

## âœ… Checklist de pasos

### ğŸ› ï¸ PreparaciÃ³n del entorno

- [X] Crear entorno virtual (`venv`, `conda`, etc.)

```
PS C:\Users\enman\Downloads\COLFONDOS> & C:/Users/enman/Downloads/COLFONDOS/.venv/Scripts/Activate.ps1
(.venv) PS C:\Users\enman\Downloads\COLFONDOS> & C:\Users\enman\Downloads\COLFONDOS\.venv\Scripts\Activate.ps1
(.venv) PS C:\Users\enman\Downloads\COLFONDOS> cd C:\Users\enman\Downloads\COLFONDOS\workana_data_engineer_project
```

- [X] Instalar dependencias: `apache-airflow`, `pandas`, `sqlalchemy`, `psycopg2` (si usas PostgreSQL)

```
(.venv) PS C:\Users\enman\Downloads\COLFONDOS\workana_data_engineer_project> pip install -r requirements.txt
```

- [X] Crear estructura de carpetas del proyecto
  - `airflow_dags/`
  - `etl/`
  - `tests/`
  - `scripts/`
  - `data/`

### ğŸ” Descarga y lectura de datos

- [X] Crear script para descargar `sample_transactions.csv`âœ `scripts/download_csv.py`
- [X] Implementar sensor de espera de archivo (con tamaÃ±o mÃ­nimo)
  âœ `etl/sensors.py`

### ğŸ§¹ TransformaciÃ³n de datos

- [X] Crear funciÃ³n de transformaciÃ³n con `pandas` (lectura por chunks opcional)
  âœ `etl/transform.py`

## ğŸ“Œ Control de versiones y push a rama remota

Durante el desarrollo de esta prueba tÃ©cnica, utilicÃ© **Git** como sistema de control de versiones y **GitHub** como repositorio remoto. Para mantener el historial de cambios limpio y reflejar el avance del proyecto, ejecutÃ© los siguientes comandos manualmente desde el entorno local:

```bash
git init
git add .
git commit -m "Initial commit - Workana Data Engineer challenge (SQLite)"
git branch -M main
git remote add origin https://github.com/ecuadrosg36/workana-data-engineer-test.git
git push -u origin main
```

### ğŸ—„ï¸ Carga a base de datos

- [X] Configurar conexiÃ³n a SQLite

![1753651696522](image/README/1753651696522.png)

- [X] Crear funciÃ³n para carga a DB
  âœ `etl/load.py`

![1753658977385](image/README/1753658977385.png)

![1753659062275](image/README/1753659062275.png)

![1753659089795](image/README/1753659089795.png)

## ğŸ§  Por quÃ© SQLite

Inicialmente se planeÃ³ usar PostgreSQL, pero se optÃ³ por SQLite como alternativa prÃ¡ctica. SQLite no requiere instalaciÃ³n, es compatible con SQLAlchemy y permite ejecutar el pipeline completo localmente.

El cÃ³digo estÃ¡ preparado para cambiar fÃ¡cilmente a PostgreSQL si se desea.

### âš™ï¸ OrquestaciÃ³n con Airflow

- [X] Crear DAG con tareas:

  - Descargar archivo
  - Esperar archivo y tamaÃ±o
  - Transformar datos
  - Cargar a DB

## ğŸ³ EjecuciÃ³n con Docker + Airflow (recomendado)

- [X] Esta configuraciÃ³n inicia Airflow (webserver + scheduler) y PostgreSQL como backend.

```
docker-compose up
```

  Esto realiza lo siguiente:

* Inicia PostgreSQL como base de datos de Airflow
* Inicializa Airflow
* Crea el usuario admin
* Expone la interfaz en: [http://localhost:8080](http://localhost:8080)

  **Credenciales de acceso:**
* Usuario: `admin`
* ContraseÃ±a: `admin`

---

## ğŸ§  Decisiones tÃ©cnicas

* **SQLite** se utiliza como base de datos de carga por simplicidad y portabilidad.
* **Airflow en Docker** permite orquestaciÃ³n reproducible sin depender del sistema operativo.
* **Modularidad y pruebas** : el proyecto estÃ¡ dividido en componentes reutilizables (`scripts/`, `etl/`, `dags/`) con logging, validaciones y manejo de errores.

  ![1753663082080](image/README/1753663082080.png)

![1753663132046](image/README/1753663132046.png)

![1753663219056](image/README/1753663219056.png)

- [X] Agregar sensores y reintentos a las tareas
  âœ `airflow_dags/etl_transactions_dag.py`

  ![1753666490954](image/README/1753666490954.png)

  ![1753718989694](image/README/1753718989694.png)

  ![1753719590824](image/README/1753719590824.png)

### ğŸ§ª Testing

- [X] Escribir tests unitarios para transformaciÃ³n y cargaâœ `tests/test_transform.py`âœ `tests/test_load.py`
- [X] Escribir test de integraciÃ³n para el DAG completo

  ![1753721641888](image/README/1753721641888.png)

### ğŸ“ˆ Logging y mÃ©tricas 

- [ ] Registrar logs detallados por tarea
- [ ] Medir tiempo de ejecuciÃ³n por paso
- [ ] Registrar cantidad de registros procesados

### ğŸ“¬ Validaciones y alertas

- [ ] Validar si tabla destino estÃ¡ vacÃ­a
- [ ] Generar alerta/log en caso de error

## ğŸš€ Extras 

- [ ] Lectura eficiente con `pandas.read_csv(..., chunksize=...)`
- [ ] Soporte para `.csv.gz` con `compression='gzip'`
- [ ] Replicar el pipeline con Prefect o Dagster y comparar resultados

# âœ… Plan de AcciÃ³n: Ejercicio 2 - SQL y anÃ¡lisis

## ğŸ¯ Objetivo

Ejecutar consultas SQL sobre los datos ya cargados en la base de datos (desde el Ejercicio 1), para generar reportes, identificar errores, y proponer mejoras de rendimiento.

---

## âœ… Checklist de pasos

### ğŸ“„ 1. Crear vista/tabla resumen por dÃ­a y estado

- [ ] Definir estructura: fecha, estado, cantidad de transacciones
- [ ] Crear script SQLâœ `sql/view_summary_by_date_status.sql`
- [ ] Ejecutar script y verificar contenido

### ğŸ” 2. Query para usuarios con >3 transacciones fallidas en Ãºltimos 7 dÃ­as

- [ ] Identificar campo `status` o equivalente para marcar transacciÃ³n fallida
- [ ] Escribir query con `GROUP BY user_id`, `HAVING COUNT > 3`, `WHERE fecha >= current_date - 7`
- [ ] Guardar como
  âœ `sql/query_frequent_failures.sql`

### ğŸ“ˆ 3. DetecciÃ³n de anomalÃ­as (incrementos anÃ³malos)

- [ ] Crear query que compare conteo diario con el promedio de dÃ­as anteriores
- [ ] Definir umbral de alerta (ej: +100% sobre la media de los Ãºltimos 3 dÃ­as)
- [ ] Guardar comoâœ `sql/query_detect_anomalies.sql`
- [ ] (Opcional) Simular alerta/log si se detecta

### ğŸ§± 4. Ãndices y triggers

- [ ] Crear Ã­ndices sobre columnas claves (ej: `user_id`, `status`, `timestamp`)âœ `sql/create_indexes.sql`
- [ ] DiseÃ±ar triggers para:
  - [ ] Detectar valores fuera de rango (ej: fecha futura)
  - [ ] Detectar intentos de inserciÃ³n duplicada
    âœ `sql/create_triggers.sql`

### ğŸ“‚ 5. Particionamiento lÃ³gico

- [ ] Documentar propuesta de particionado (ej: por mes usando `timestamp`)
- [ ] Simularlo si DB lo permite (ej: `CREATE TABLE datos_2025_07` + `UNION ALL`)
- [ ] Explicar ventajas esperadas en rendimiento

---

## ğŸ—‚ï¸ Enlaces por hacer / dependencias

- [ ] [ ] Asegurar que base de datos contiene los datos cargados del ejercicio 1
- [ ] [ ] Crear conexiÃ³n desde SQL scripts o notebook a la DB
- [ ] [ ] Documentar cada query en el README con ejemplo de uso y output esperado

---

## ğŸ“ Extras (si hay tiempo)

- [ ] Automatizar queries en script Python con SQLAlchemy o `psycopg2`
- [ ] Graficar resultados con `matplotlib` o `plotly` (opcional)
- [ ] Simular monitoreo con logs diarios

# âœ… Plan de AcciÃ³n: Ejercicio 3 - ETL Python para archivo grande

## ğŸ¯ Objetivo

Procesar un archivo `sample.log.gz` (~5 millones de lÃ­neas en formato JSONL) en modo streaming, filtrando errores y generando mÃ©tricas agregadas por hora y endpoint.

---

## âœ… Checklist de pasos

### ğŸ“¦ 1. Preparar entorno y archivo

- [ ] Descomprimir o validar lectura directa del archivo `.gz`
- [ ] Validar estructura JSONL: una lÃ­nea = un JSON
- [ ] Crear carpeta para scripts ETL
  âœ `etl/large_log_etl.py`

### ğŸ§¾ 2. Leer archivo por streaming

- [ ] Implementar lectura lÃ­nea a lÃ­nea desde `.gz`âœ Usar `gzip.open()` + `json.loads()`
- [ ] Filtrar solo lÃ­neas con `status_code >= 500`
- [ ] Manejar errores de parseo JSON con try/except

### ğŸ§¹ 3. Limpiar y parsear campos

- [ ] Validar y limpiar campos clave (`timestamp`, `endpoint`, `status_code`)
- [ ] Convertir timestamp a datetime y redondear por hora

### ğŸ“Š 4. Agregaciones por hora y endpoint

- [ ] Agrupar por `(hora, endpoint)` y calcular:
  - [ ] Total requests
  - [ ] Total con errores
  - [ ] Porcentaje de error
- [ ] Guardar en estructura tipo `pandas.DataFrame`

### ğŸ’¾ 5. Exportar resultados

- [ ] Exportar resultados a archivo Parquet comprimido (`Snappy`)âœ `output/errors_summary.parquet`
- [ ] Usar exportaciÃ³n por chunks si el DataFrame es muy grande

### âš™ï¸ 6. Performance y escalabilidad

- [ ] Implementar versiÃ³n alternativa con `multiprocessing`
- [ ] Probar versiones con `polars` y/o `dask`
- [ ] Medir tiempos de ejecuciÃ³n y uso de memoria (profiling)

### ğŸ› 7. Logging y manejo de errores

- [ ] Configurar `logging` para registrar:
  - [ ] Errores de parseo
  - [ ] MÃ©tricas por batch
  - [ ] Tiempos de proceso
- [ ] Guardar logs en archivo (`logs/etl_run.log`)

---

## ğŸ—‚ï¸ Enlaces por hacer / dependencias

- [ ] [ ] Validar ruta del archivo `sample.log.gz`
- [ ] [ ] Crear carpeta `output/` para Parquet
- [ ] [ ] Crear carpeta `logs/` para logs
- [ ] [ ] Agregar script al README con cÃ³mo correrlo

---

## ğŸ§ª Extras (si hay tiempo)

- [ ] Escribir pruebas unitarias para funciones clave (parsing, filtrado, agregaciÃ³n)
- [ ] Graficar las mÃ©tricas por hora con `matplotlib` o `seaborn`
- [ ] Integrar este ETL como parte del DAG general

# âœ… Plan de AcciÃ³n: Ejercicio 4 - Modelado de Datos

## ğŸ¯ Objetivo

DiseÃ±ar un modelo dimensional (estrella o copo de nieve) para representar las transacciones del archivo CSV, poblar las tablas desde los datos crudos, e implementar buenas prÃ¡cticas de modelado analÃ­tico.

---

## âœ… Checklist de pasos

### ğŸ§± 1. DiseÃ±o del modelo dimensional

- [ ] Identificar campos para la **tabla de hechos**: transacciones (ej: `monto`, `fecha`, `estado`, `user_id`)
- [ ] Identificar posibles **dimensiones**:
  - DimensiÃ³n tiempo (fecha/hora)
  - DimensiÃ³n usuario
  - DimensiÃ³n estado de transacciÃ³n
- [ ] Elegir entre modelo **estrella (star)** o **copo de nieve (snowflake)** y justificar decisiÃ³n
- [ ] Diagramar el modelo (opcional con [dbdiagram.io](https://dbdiagram.io) o similar)

### ğŸ—„ï¸ 2. CreaciÃ³n de las tablas

- [ ] Crear script SQL para tablas:
  - `fact_transacciones`
  - `dim_usuario`
  - `dim_fecha`
  - `dim_estado`
- [ ] Usar claves primarias en dimensiones y claves forÃ¡neas en la tabla de hechos
  âœ `sql/model_tables.sql`

### ğŸš€ 3. Carga inicial desde CSV

- [ ] Escribir script Python para poblar dimensiones y hechos desde `sample_transactions.csv`âœ `etl/load_model_data.py`
- [ ] Asegurar inserciÃ³n sin duplicados en dimensiones
- [ ] Resolver dependencias entre tablas (poblar primero dimensiones)

### ğŸŒ€ 4. Estrategia SCD (Slowly Changing Dimensions)

- [ ] Elegir e implementar SCD Tipo 1 o Tipo 2 en al menos una dimensiÃ³n (ej: `dim_usuario`)
- [ ] Documentar cÃ³mo se maneja el historial y actualizaciones
- [ ] Incluir campos `is_current`, `valid_from`, `valid_to` si es SCD Tipo 2

### ğŸ“¦ 5. Particionamiento lÃ³gico y archivado

- [ ] Simular particiÃ³n lÃ³gica de la tabla de hechos por mes (`YYYYMM`)
- [ ] Documentar ventajas para el rendimiento (lectura selectiva, limpieza, mantenimiento)
- [ ] Simular archivado (ej: mover datos viejos a otra tabla o archivo externo)

### âš™ï¸ 6. Performance y optimizaciÃ³n

- [ ] Crear Ã­ndices sobre claves forÃ¡neas y campos de filtro frecuentes (`fecha`, `estado`)
- [ ] Documentar decisiones de optimizaciÃ³n (densidad de claves, Ã­ndices, orden de carga)
- [ ] Validar que las relaciones estÃ©n bien formadas

---

## ğŸ—‚ï¸ Enlaces por hacer / dependencias

- [ ] [ ] Confirmar estructura del archivo `sample_transactions.csv`
- [ ] [ ] Crear esquema de base de datos nuevo para el modelo (opcional)
- [ ] [ ] Agregar al README: esquema, scripts y ejemplo de queries sobre el modelo

---

## ğŸ§ª Extras (si hay tiempo)

- [ ] Probar queries analÃ­ticas sobre la tabla de hechos (ej: KPIs por usuario/mes)
- [ ] Automatizar carga con Airflow
- [ ] Exportar modelo como `.erdiagram`, `.pdf` o imagen

# âœ… Plan de AcciÃ³n: Ejercicio 5 - Git + CI/CD

## ğŸ¯ Objetivo

Organizar todo el proyecto en un repositorio Git con estructura clara y reproducible, incluyendo automatizaciÃ³n de pruebas, chequeos de calidad, y (opcionalmente) contenedores y orquestaciÃ³n local.

---

## âœ… Checklist de pasos

### ğŸ“ 1. Estructura modular del repositorio

- [ ] Crear las siguientes carpetas base:
  - `airflow/` â†’ DAGs y configuraciÃ³n de Airflow
  - `etl/` â†’ scripts ETL generales
  - `sql/` â†’ scripts SQL y creaciÃ³n de modelo
  - `modeling/` â†’ documentaciÃ³n del modelo dimensional
  - `data/` â†’ archivos de entrada, comprimidos y resultados
  - `ci/` â†’ archivos de configuraciÃ³n para CI
  - `tests/` â†’ pruebas unitarias e integraciÃ³n
  - `logs/` y `output/` â†’ salidas de ejecuciÃ³n
- [ ] Agregar archivos `README.md` y `.gitignore` por carpeta si es necesario

### ğŸŒ¿ 2. Estrategia de branches

- [ ] Definir ramas principales:
  - `main` â†’ rama estable y productiva
  - `dev` â†’ rama de integraciÃ³n y pruebas
  - `feature/<nombre>` â†’ desarrollo de nuevas funciones (ej: `feature/etl-error-logs`)
- [ ] Configurar protecciones para `main` y `dev` (opcional en GitHub/GitLab)

### ğŸ”„ 3. AutomatizaciÃ³n con CI (GitHub Actions o GitLab CI)

- [ ] Crear workflow de CI/CD en `.github/workflows/ci.yml` o `.gitlab-ci.yml`
- [ ] Incluir en el pipeline:
  - [ ] Linter (`flake8`, `black`, `isort`)
  - [ ] Tests (`pytest`)
  - [ ] Chequeos estÃ¡ticos (`mypy`, `bandit`)
  - [ ] ValidaciÃ³n de DAGs (si se usa Airflow)
  - [ ] Reporte de cobertura (opcional con `coverage`)

### ğŸ³ 4. Docker y ejecuciÃ³n reproducible (opcional)

- [ ] Crear `Dockerfile` para ejecutar ETL o DAGs localmente
- [ ] Crear `docker-compose.yml` si hay mÃºltiples servicios (Airflow + DB)
- [ ] Documentar cÃ³mo correr el entorno en README
- [ ] Incluir rollback (ej: scripts para revertir carga) y reporting si es posible

---

## ğŸ—‚ï¸ Enlaces por hacer / dependencias

- [ ] [ ] Conectar CI al repositorio en GitHub/GitLab
- [ ] [ ] Crear archivos de configuraciÃ³n: `.pre-commit-config.yaml`, `pyproject.toml`
- [ ] [ ] Agregar al README: instrucciones para contribuir, correr tests, y ejecutar CI

---

## ğŸ“œ Entregables mÃ­nimos

- [ ] README claro con estructura del proyecto y pasos para ejecuciÃ³n
- [ ] Logs y evidencia de ejecuciÃ³n en `logs/` o adjuntos
- [ ] Scripts y notebooks versionados
- [ ] Pipeline CI funcionando o documentado

---

## ğŸ§ª Extras (si hay tiempo)

- [ ] IntegraciÃ³n con Notebooks o documentaciÃ³n automatizada (ej: MkDocs)
- [ ] Reportes visuales o dashboards con mÃ©tricas
- [ ] Pipeline para publicar imagen Docker o paquete Python
